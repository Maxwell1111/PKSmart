{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af10af7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from numpy import argmax\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.experimental import enable_halving_search_cv\n",
    "from sklearn.model_selection import HalvingRandomSearchCV\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import cross_val_score \n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import collections\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.inspection import permutation_importance\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abce8ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed38a9cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "animal = pd.read_csv(\"../data/animal_smiles_r_v3.csv\")\n",
    "animal = animal[animal[\"smiles_r\"]!=\"Cannot_do\"].reset_index(drop=True)\n",
    "\n",
    "rat = animal[[\"InChICode_standardised\",  \"smiles_r\", \"rat_VDss_L_kg\", \"rat_CL_mL_min_kg\", \"rat_fup\"]].dropna(subset=[\"rat_VDss_L_kg\", \"rat_CL_mL_min_kg\", \"rat_fup\"], how=\"all\").reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ccf91b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "rat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eb13e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "inchi = rat[[\"InChICode_standardised\", \"smiles_r\"]]\n",
    "for endpoint in [\"rat_VDss_L_kg\",\"rat_CL_mL_min_kg\"]:\n",
    "    print(\"Transformed endpoint to log base 10\")\n",
    "    rat[endpoint] = np.log10(rat[endpoint])\n",
    "rat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85eaad75",
   "metadata": {},
   "outputs": [],
   "source": [
    "rat = rat.groupby('smiles_r').median().reset_index()\n",
    "rat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c4b6fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from itertools import compress\n",
    "\n",
    "def fs_variance(df, threshold:float=0.05):\n",
    "    \"\"\"\n",
    "    Return a list of selected variables based on the threshold.\n",
    "    \"\"\"\n",
    "\n",
    "    # The list of columns in the data frame\n",
    "    features = list(df.columns)\n",
    "    \n",
    "    # Initialize and fit the method\n",
    "    vt = VarianceThreshold(threshold = threshold)\n",
    "    _ = vt.fit(df)\n",
    "    \n",
    "    # Get which column names which pass the threshold\n",
    "    feat_select = list(compress(features, vt.get_support()))\n",
    "    \n",
    "    return feat_select\n",
    "\n",
    "def get_pairwise_correlation(population_df, method=\"pearson\"):\n",
    "    \"\"\"Given a population dataframe, calculate all pairwise correlations.\n",
    "    Parameters\n",
    "    ----------\n",
    "    population_df : pandas.core.frame.DataFrame\n",
    "        Includes metadata and observation features.\n",
    "    method : str, default \"pearson\"\n",
    "        Which correlation matrix to use to test cutoff.\n",
    "    Returns\n",
    "    -------\n",
    "    list of str\n",
    "        Features to exclude from the population_df.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    # Get a symmetrical correlation matrix\n",
    "    data_cor_df = population_df.corr(method=method)\n",
    "\n",
    "    # Create a copy of the dataframe to generate upper triangle of zeros\n",
    "    data_cor_natri_df = data_cor_df.copy()\n",
    "\n",
    "    # Replace upper triangle in correlation matrix with NaN\n",
    "    data_cor_natri_df = data_cor_natri_df.where(\n",
    "        np.tril(np.ones(data_cor_natri_df.shape), k=-1).astype(np.bool)\n",
    "    )\n",
    "\n",
    "    # Acquire pairwise correlations in a long format\n",
    "    # Note that we are using the NaN upper triangle DataFrame\n",
    "    pairwise_df = data_cor_natri_df.stack().reset_index()\n",
    "    pairwise_df.columns = [\"pair_a\", \"pair_b\", \"correlation\"]\n",
    "\n",
    "    return data_cor_df, pairwise_df\n",
    "\n",
    "def determine_high_cor_pair(correlation_row, sorted_correlation_pairs):\n",
    "    \"\"\"\n",
    "    Select highest correlated variable given a correlation row with columns:\n",
    "    [\"pair_a\", \"pair_b\", \"correlation\"]\n",
    "    For use in a pandas.apply()\n",
    "    \"\"\"\n",
    "\n",
    "    pair_a = correlation_row[\"pair_a\"]\n",
    "    pair_b = correlation_row[\"pair_b\"]\n",
    "\n",
    "    if sorted_correlation_pairs.get_loc(pair_a) > sorted_correlation_pairs.get_loc(pair_b):\n",
    "        return pair_a\n",
    "    \n",
    "    else:\n",
    "        return pair_b\n",
    "    \n",
    "def count(pred, true, min, max, endpoint):\n",
    "\n",
    "    if(endpoint == \"rat_fup\"):\n",
    "        lst = [abs(a/b) for a, b in zip(pred, true)]\n",
    "        #print(lst)\n",
    "\n",
    "        newlist = [x for x in lst if min <= x <= max]\n",
    "        #print(newlist)\n",
    "\n",
    "        return (len(newlist)/len(lst)) *100\n",
    "    \n",
    "    else:\n",
    "        lst = [abs(10**a/10**b) for a, b in zip(pred, true)]\n",
    "        #print(lst)\n",
    "\n",
    "        newlist = [x for x in lst if min <= x <= max]\n",
    "        #print(newlist)\n",
    "\n",
    "        return (len(newlist)/len(lst)) *100\n",
    "    \n",
    "    return\n",
    "\n",
    "def calc_gmfe(pred, true, endpoint):\n",
    "    \n",
    "    if(endpoint == \"rat_fup\"):\n",
    "        \n",
    "        lst = [abs(np.log10(a/b)) for a, b in zip(pred, true)]\n",
    "        mean_abs= np.mean(lst)\n",
    "        return (10**mean_abs)\n",
    "    \n",
    "    else: \n",
    "        lst = [abs(np.log10(10**a/10**b)) for a, b in zip(pred, true)]\n",
    "        mean_abs= np.mean(lst)\n",
    "        \n",
    "        return (10** mean_abs)\n",
    "    \n",
    "    return\n",
    "                         \n",
    "def median_fold_change_error(pred, true, endpoint):\n",
    "                         \n",
    "    if (endpoint == \"rat_fup\"):\n",
    "        lst = [abs(np.log10(a/b)) for a, b in zip(pred, true)]\n",
    "        median_abs= np.median(lst) \n",
    "        return (np.e**median_abs)\n",
    "    \n",
    "    else:\n",
    "        lst = [abs(np.log10(10**a/10**b)) for a, b in zip(pred, true)]\n",
    "        median_abs= np.median(lst) \n",
    "        return (np.e**median_abs)\n",
    "    \n",
    "    return\n",
    "                          \n",
    "def calc_bias(pred, true, endpoint):\n",
    "                          \n",
    "    if (endpoint == \"rat_fup\"):\n",
    "        lst = [(a - b) for a, b in zip(pred, true)]                 \n",
    "        bias= np.median(lst)\n",
    "        return bias\n",
    "    \n",
    "    else:\n",
    "        lst = [(10**a - 10**b) for a, b in zip(pred, true)]                 \n",
    "        bias= np.median(lst)\n",
    "        return bias\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d39b6220",
   "metadata": {},
   "outputs": [],
   "source": [
    "smiles_r = rat[[\"smiles_r\"]]\n",
    "smiles_r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7715fadc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdkit import Chem\n",
    "from mordred import Calculator, descriptors\n",
    "\n",
    "# create descriptor calculator with all descriptors\n",
    "calc = Calculator(descriptors, ignore_3D=True)\n",
    "\n",
    "print(len(calc.descriptors))\n",
    "\n",
    "Ser_Mol = rat['smiles_r'].apply(Chem.MolFromSmiles)\n",
    "\n",
    "# as pandas\n",
    "Mordred_table=  calc.pandas(Ser_Mol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc9eec82",
   "metadata": {},
   "outputs": [],
   "source": [
    "Mordred_table = Mordred_table.astype('float')\n",
    "Mordred_table['smiles_r'] = rat['smiles_r']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c024176",
   "metadata": {},
   "outputs": [],
   "source": [
    "Mordred_table = Mordred_table.dropna(axis='columns')\n",
    "selected_Features = fs_variance(Mordred_table .iloc[:,:-1], threshold=0.05)\n",
    "print(len(selected_Features))\n",
    "new_Mordred_table = Mordred_table [selected_Features]\n",
    "# Get correlation matrix and lower triangle of pairwise correlations in long format\n",
    "data_cor_df, pairwise_df = get_pairwise_correlation(population_df=new_Mordred_table)\n",
    "# Get absolute sum of correlation across features\n",
    "# The lower the index, the less correlation to the full data frame\n",
    "# We want to drop features with highest correlation, so drop higher index\n",
    "variable_cor_sum = data_cor_df.abs().sum().sort_values().index\n",
    "# And subset to only variable combinations that pass the threshold\n",
    "pairwise_df = pairwise_df.query(\"correlation > 0.95\")\n",
    "excluded = pairwise_df.apply(lambda x: determine_high_cor_pair(x, variable_cor_sum), axis=\"columns\")\n",
    "excluded_features = list(set(excluded.tolist()))\n",
    "print(len(excluded_features))\n",
    "new_Mordred_table = new_Mordred_table.drop(excluded_features, axis=1)\n",
    "max_feature_values = new_Mordred_table.max().abs()\n",
    "min_feature_values = new_Mordred_table.min().abs()\n",
    "#outlier_features = max_feature_values[(max_feature_values > 15) | (min_feature_values > 15)].index.tolist()\n",
    "#print(len(outlier_features))\n",
    "#new_Mordred_table = new_Mordred_table.drop(outlier_features, axis=1)\n",
    "new_Mordred_table['smiles_r'] = rat['smiles_r']\n",
    "new_Mordred_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "421a0c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ser_Mol = rat['smiles_r'].apply(Chem.MolFromSmiles)\n",
    "\n",
    "import numpy as np\n",
    "from rdkit.Chem.rdMolDescriptors import GetMorganFingerprintAsBitVect\n",
    "\n",
    "Morgan_fingerprint= Ser_Mol.apply(GetMorganFingerprintAsBitVect, args=(2, 2048))\n",
    "Morganfingerprint_array  = np.stack(Morgan_fingerprint)\n",
    "\n",
    "Morgan_collection  = []\n",
    "for x in np.arange(Morganfingerprint_array.shape[1]): #np.arange plus rapide que range\n",
    "    x = \"Mfp\"+str(x)\n",
    "    Morgan_collection.append(x)\n",
    "\n",
    "Morganfingerprint_table  = pd.DataFrame(Morganfingerprint_array , columns=Morgan_collection )\n",
    "Morganfingerprint_table['smiles_r'] = rat['smiles_r']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0559f6a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_Features = fs_variance(Morganfingerprint_table.iloc[:,:-1], threshold=0.05)\n",
    "print(len(selected_Features))\n",
    "new_Morganfingerprint_table= Morganfingerprint_table[selected_Features]\n",
    "new_Morganfingerprint_table['smiles_r'] = rat['smiles_r']\n",
    "new_Morganfingerprint_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f3cee60",
   "metadata": {},
   "outputs": [],
   "source": [
    "rat_mfp = pd.merge(rat, new_Morganfingerprint_table)\n",
    "rat_mfp_Mordred = pd.merge(rat_mfp, new_Mordred_table)\n",
    "\n",
    "# Load CE50 predictions\n",
    "ce50_predictions = pd.read_csv('data/rat_ce50_predictions_simple.csv')\n",
    "ce50_features = ce50_predictions[['smiles_r', 'ce50', 'pce50', 'confidence']]\n",
    "\n",
    "# Merge CE50 features with rat_mfp_Mordred\n",
    "rat_mfp_Mordred = pd.merge(rat_mfp_Mordred, ce50_features, on='smiles_r', how='left')\n",
    "\n",
    "rat_mfp_Mordred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e859d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "Mordred_columns = new_Mordred_table.columns[:-1].tolist()\n",
    "mfp_columns = new_Morganfingerprint_table.columns[:-1].tolist()\n",
    "\n",
    "# Add CE50 features to the feature list\n",
    "ce50_columns = ['ce50', 'pce50', 'confidence']\n",
    "features_mfp_mordred_columns = Mordred_columns + mfp_columns + ce50_columns\n",
    "print(len(features_mfp_mordred_columns))\n",
    "\n",
    "#Write to file\n",
    "f = open(\"features_mfp_mordred_ce50_columns_rat_model.txt\", \"w\")\n",
    "for item in features_mfp_mordred_columns:\n",
    "   f.write(item + \"\\n\")\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd0985cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#read from file\n",
    "file = open(\"features_mfp_mordred_ce50_columns_rat_model.txt\", \"r\")\n",
    "file_lines = file.read()\n",
    "features_mfp_mordred_columns = file_lines.split(\"\\n\")\n",
    "features_mfp_mordred_columns = features_mfp_mordred_columns[:-1]\n",
    "features_mfp_mordred_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49799317",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "import pickle\n",
    "list_of_lists=[]\n",
    "\n",
    "for endpoint in [\"rat_VDss_L_kg\",\"rat_CL_mL_min_kg\",\"rat_fup\"]:\n",
    "    \n",
    "    baseline=10.00\n",
    "    #log human_VDss_L_kg model\n",
    "    data = rat_mfp_Mordred\n",
    "    features = features_mfp_mordred_columns\n",
    "        \n",
    "\n",
    "    print(endpoint)    \n",
    "    df = data.dropna(subset=[endpoint]).reset_index(drop=True)\n",
    "\n",
    "    X = df[features]\n",
    "    Y = df[endpoint]\n",
    "         \n",
    "    # Scale the features\n",
    "    scaler = StandardScaler()\n",
    "    X = scaler.fit_transform(X)\n",
    "    X = pd.DataFrame(X, columns=features)\n",
    "    \n",
    "    for i in range(42,47):\n",
    "        \n",
    "\n",
    "        #Nested Cross Validation    \n",
    "        outercv = KFold(n_splits=5, random_state=i, shuffle=True) \n",
    "\n",
    "        for split, (train_index, test_index) in enumerate(outercv.split(X)):\n",
    "            print(split)\n",
    "\n",
    "            X_train= X.iloc[train_index].values\n",
    "            Y_train= Y.iloc[train_index].values.flatten()\n",
    "\n",
    "            X_test= X.iloc[test_index].values\n",
    "            Y_test= Y.iloc[test_index].values.flatten()\n",
    "\n",
    "            print(X_train.shape)\n",
    "            print(X_test.shape)\n",
    "\n",
    "            inner_cv = KFold(n_splits=4, random_state=i, shuffle=True) \n",
    "\n",
    "            # Create a based model\n",
    "            regressor = RandomForestRegressor(random_state = i, n_jobs=20)\n",
    "\n",
    "            # Instantiate the RandomHalving search model\n",
    "            param_grid = { \n",
    "            \"n_estimators\": [100,200,300],\n",
    "            \"max_features\": [\"sqrt\", \"log2\"],\n",
    "            \"min_samples_split\": [2,4,8],\n",
    "            \"max_depth\": [5, 10, 15],    \n",
    "            \"bootstrap\": [True, False],\n",
    "            'n_jobs': [30]\n",
    "            }\n",
    "\n",
    "            gsh = GridSearchCV(estimator = regressor, param_grid = param_grid, cv = inner_cv, n_jobs=30, verbose = 1)\n",
    "\n",
    "            ##MODELS\n",
    "            print(f\"Running ____________________{endpoint} MODELS\")\n",
    "\n",
    "            gsh.fit(X_train, Y_train)\n",
    "            #print(gsh.best_estimator_)\n",
    "\n",
    "            #HeldOutTest\n",
    "            classifier = gsh.best_estimator_\n",
    "            print(classifier)\n",
    "            classifier.fit(X_train, Y_train)\n",
    "\n",
    "            y_pred =  classifier.predict(X_test)     \n",
    "            print(len(y_pred))\n",
    "\n",
    "            fold_2= count(y_pred, Y_test, 0.5, 2, endpoint)\n",
    "            fold_3= count(y_pred, Y_test, 1/3, 3, endpoint)\n",
    "            fold_5= count(y_pred, Y_test, 1/5, 5, endpoint)\n",
    "            gmfe = calc_gmfe(y_pred, Y_test, endpoint)\n",
    "            mfe = median_fold_change_error(y_pred, Y_test, endpoint)\n",
    "            bias = calc_bias(y_pred, Y_test, endpoint)\n",
    "\n",
    "            print(\"2-fold : \", fold_2)\n",
    "            print(\"3-fold : \", fold_3)\n",
    "            print(\"5-fold : \", fold_5)\n",
    "            print(\"gmfe : \", gmfe)\n",
    "            print(\"mfe : \", mfe)\n",
    "            print(\"bias : \", bias)\n",
    "\n",
    "            rmse = sqrt(mean_squared_error(Y_test, y_pred))\n",
    "            print('rmse ',rmse)\n",
    "            r2 = r2_score(Y_test, y_pred)\n",
    "            print('r2 ',r2)\n",
    "\n",
    "            #if (r2>baseline):\n",
    "            if (gmfe<baseline):    \n",
    "                # save\n",
    "                print(\"saving model\")\n",
    "                baseline=gmfe\n",
    "                pickle.dump(classifier, open(f\"log_{endpoint}_ce50_model.sav\", 'wb'))\n",
    "\n",
    "            sns.set_theme(style=\"ticks\")\n",
    "            df=pd.DataFrame({\"y_pred\":y_pred, \"Y_test\":Y_test})\n",
    "            sns.lmplot(data=df, x=\"y_pred\", y=\"Y_test\")\n",
    "            plt.show()\n",
    "\n",
    "            row=[\"HeldOut\",  endpoint, i, split, fold_2, fold_3, fold_5, gmfe, mfe, bias, rmse, r2]\n",
    "            list_of_lists.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03efbeb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame(list_of_lists, columns=[\"HeldOut\", \"endpoint\", \"random_state\", \"split\", \"fold_2\", \"fold_3\", \"fold_5\", \"gmfe\", \"mfe\", \"bias\", \"rmse\", \"r2\"])\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9dfd992",
   "metadata": {},
   "outputs": [],
   "source": [
    "results.to_csv(\"Prediction_rat_from_mordred_morgan_fs_ce50.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89f47b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "results= pd.read_csv(\"Prediction_rat_from_mordred_morgan_fs_ce50.csv\")\n",
    "results.groupby(\"endpoint\").mean().reset_index().to_csv(\"rat_ce50.csv\", index=False)\n",
    "results.groupby(\"endpoint\").mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e050650",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load\n",
    "loaded_rf = pickle.load(open(\"log_rat_VDss_L_kg_ce50_model.sav\", 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf3a7a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af032415",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "list_of_lists=[]\n",
    "\n",
    "\n",
    "for endpoint in [\"rat_VDss_L_kg\",\"rat_CL_mL_min_kg\",\"rat_fup\"]:\n",
    "    \n",
    "    baseline=10.00\n",
    "    #log human_VDss_L_kg model\n",
    "    data = rat_mfp_Mordred\n",
    "    features = features_mfp_mordred_columns\n",
    "        \n",
    "\n",
    "    print(endpoint)    \n",
    "    df = data.dropna(subset=[endpoint]).reset_index(drop=True)\n",
    "\n",
    "    X = df[features]\n",
    "    Y = df[endpoint]\n",
    "\n",
    "    # Scale the features\n",
    "    scaler = StandardScaler()\n",
    "    X = scaler.fit_transform(X)\n",
    "    X = pd.DataFrame(X, columns=features)\n",
    "    \n",
    "    # Save the scaler to a file\n",
    "    with open('scaler_rat_ce50.pkl', 'wb') as file:\n",
    "        pickle.dump(scaler, file)\n",
    "    \n",
    "    X_train= X.values\n",
    "    Y_train= Y.values.flatten()\n",
    "\n",
    "    print(X_train.shape)\n",
    "    print(X_test.shape)\n",
    "\n",
    "          \n",
    "    loaded_rf = pickle.load(open(f\"log_{endpoint}_ce50_model.sav\", 'rb'))\n",
    "\n",
    "    \n",
    "    params = loaded_rf.get_params()\n",
    "    \n",
    "    classifier= RandomForestRegressor(**params)\n",
    "    classifier.fit(X_train, Y_train)\n",
    "    \n",
    "    pickle.dump(classifier, open(f\"log_{endpoint}_ce50_model_FINAL.sav\", 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc24908",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e662890",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
